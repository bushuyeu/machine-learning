{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEpzXW6RnJy"
      },
      "source": [
        "# Fisher Discriminants\n",
        "\n",
        "**Notation and setup:**\n",
        "\n",
        "- $X \\in \\mathbb{R}^{n \\times p}$: matrix, each row being an example in the training set ($p$ features)\n",
        "- Two classes: $n_1$ examples labeled $+1$, and $n_2$ examples labeled $-1$\n",
        "- $n = n_1 + n_2 > p$\n",
        "- $m_1, m_2 \\in \\mathbb{R}^p$: class means\n",
        "- $y \\in \\{+1,-1\\}^n$: label vector\n",
        "- $d := m_1 - m_2$: direction between class means\n",
        "- $S_b := dd^T$: between-class scatter matrix"
      ],
      "id": "scEpzXW6RnJy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1)\n",
        "Let $u$ be any unit vector in $\\mathbb{R}^p$ (that is, a vector with $p$ coordinates and length $1$). If $x$ (another $p$-coordinate vector), what is the projection of $x$ on to the line through the origin containing the vector $u$? We will call this the projection of $x$ onto $u$ onto for convenience in notation."
      ],
      "metadata": {
        "id": "WjfhdV-SaUJ5"
      },
      "id": "WjfhdV-SaUJ5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McR4WT2GRnJ1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "For a unit vector $u \\in \\mathbb{R}^p$ and any $x \\in \\mathbb{R}^p$ the vector projection onto the line through the origin spanned by $u$ is\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\operatorname{proj}_u(x) = (u^T x)\\,u\n",
        "$$"
      ],
      "id": "McR4WT2GRnJ1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2)\n",
        "Show that the squared distance between the projections of $m_1$ and $m_2$ onto $u$ is $u^TS_bu$, where $S_b = (m_1 - m_2)(m_1 - m_2)^T$"
      ],
      "metadata": {
        "id": "dw5JbA5BaRfX"
      },
      "id": "dw5JbA5BaRfX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWV-1wDORnJ2"
      },
      "source": [
        "\n",
        "\n",
        "$$\n",
        "\\|\\operatorname{proj}_u(m_1)-\\operatorname{proj}_u(m_2)\\|^2\n",
        "= \\|(u^Tm_1)u-(u^Tm_2)u\\|^2\n",
        "= |u^T(m_1-m_2)|^2 \\cdot \\underbrace{\\|u\\|^2}_{=1}\n",
        "= (u^Td)^2.\n",
        "$$\n",
        "\n",
        "Writing this in quadratic form using $S_b = dd^T$:\n",
        "\n",
        "$$\n",
        "(u^Td)^2 = (u^Td)(d^Tu) = u^T\\underbrace{dd^T}_{S_b}u = u^TS_bu.\n",
        "$$\n",
        "\n",
        "So the between-class scatter along $u$ is $u^TS_bu$."
      ],
      "id": "hWV-1wDORnJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3)\n",
        "\n",
        "Let $x$ belong to class $+1$ (say), which has mean $m_1$ as noted earlier. The squared distance between the projection of $x$ and that of $m_1$ onto $u$ is\n",
        "$$\n",
        "\\left(u^{T}(x-m_1)\\right)^2.\n",
        "$$\n",
        "\n",
        "Let $C_1$ be the set of all vectors in class $+1$. The scatter for class $+1$ along $u$ is defined to be\n",
        "$$\n",
        "\\sum_{x\\in C_1} \\left(u^{T}(x-m_1)\\right)^2.\n",
        "$$\n",
        "\n",
        "Similarly let $C_2$ be the set of all vectors with label $-1$. The scatter for class $-1$ along $u$ is defined to be\n",
        "$$\n",
        "\\sum_{x\\in C_2} \\left(u^{T}(x-m_2)\\right)^2.\n",
        "$$\n",
        "\n",
        "The total scatter within classes is the sum of the scatters for each class. Show that the total scatter within classes along $u$ is\n",
        "$$\n",
        "u^{T} S_w\\, u,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "S_w = X^{T}X - n_1\\, m_1 m_1^{T} - n_2\\, m_2 m_2^{T}.\n",
        "$$"
      ],
      "metadata": {
        "id": "GFVdfzj8ag87"
      },
      "id": "GFVdfzj8ag87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVcyMjmCRnJ2"
      },
      "source": [
        "\n",
        "For a single point $x$ in class $+1$, the squared distance from its projection to the projected mean is $(u^T(x-m_1))^2$. Summing over all points in class $+1$:\n",
        "\n",
        "$$\n",
        "\\sum_{x\\in C_1}\\big(u^T(x-m_1)\\big)^2.\n",
        "$$\n",
        "\n",
        "Use the scalar identity $(u^Ta)^2 = (u^Ta)(a^Tu) = u^T(aa^T)u$ (valid as $u^Ta$ is $a 1\\times 1$ scalar).\n",
        "\n",
        "With $a=x-m_1$, we get\n",
        "$$\n",
        "\\big(u^T(x-m_1)\\big)^2 = u^T\\big((x-m_1)(x-m_1)^T\\big)u.\n",
        "$$\n",
        "\n",
        "Therefore\n",
        "$$\n",
        "\\sum_{x\\in C_1}\\big(u^T(x-m_1)\\big)^2\n",
        "= \\sum_{x\\in C_1} u^T\\big((x-m_1)(x-m_1)^T\\big)u\n",
        "= u^T\\left(\\sum_{x\\in C_1}(x-m_1)(x-m_1)^T\\right)u.\n",
        "$$\n",
        "\n",
        "Similarly for class -1.\n",
        "\n",
        "Hence the total within-class scatter along u is\n",
        "$$\n",
        "u^T S_w u,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "S_w\n",
        "= \\sum_{x\\in C_1}(x-m_1)(x-m_1)^T + \\sum_{x\\in C_2}(x-m_2)(x-m_2)^T.\n",
        "$$\n",
        "\n",
        "Now simplify each class term  \n",
        "\n",
        "For any class $k\\in\\{1,2\\}$, expand the outer product:\n",
        "$$\n",
        "(x-m_k)(x-m_k)^T\n",
        "= xx^T - x m_k^T - m_k x^T + m_k m_k^T.\n",
        "$$\n",
        "\n",
        "Sum over $x\\in C_k$:\n",
        "$$\n",
        "\\sum_{x\\in C_k}(x-m_k)(x-m_k)^T\n",
        "= \\sum_{x\\in C_k}xx^T\n",
        "- \\sum_{x\\in C_k}x\\,m_k^T\n",
        "- \\sum_{x\\in C_k}m_k x^T\n",
        "+ \\sum_{x\\in C_k} m_k m_k^T.\n",
        "$$\n",
        "\n",
        "Pull out the constants $m_k$ and $m_k^T$:\n",
        "$$\n",
        "\\sum_{x\\in C_k}(x-m_k)(x-m_k)^T\n",
        "= \\sum_{x\\in C_k}xx^T\n",
        "- \\left(\\sum_{x\\in C_k}x\\right)m_k^T\n",
        "- m_k\\left(\\sum_{x\\in C_k}x\\right)^T\n",
        "+ n_k\\, m_k m_k^T.\n",
        "$$\n",
        "\n",
        "Use $\\sum_{x\\in C_k} x = n_k m_k$:\n",
        "$$\n",
        "\\sum_{x\\in C_k}(x-m_k)(x-m_k)^T\n",
        "= \\sum_{x\\in C_k}xx^T\n",
        "- (n_k m_k)m_k^T\n",
        "- m_k(n_k m_k^T)\n",
        "+ n_k\\, m_k m_k^T.\n",
        "$$\n",
        "\n",
        "Combine the last three terms:\n",
        "$$\n",
        "-(n_k m_k)m_k^T - m_k(n_k m_k^T) + n_k m_k m_k^T\n",
        "= -n_k m_k m_k^T.\n",
        "$$\n",
        "\n",
        "So\n",
        "$$\n",
        "\\sum_{x\\in C_k}(x-m_k)(x-m_k)^T\n",
        "= \\sum_{x\\in C_k}xx^T - n_k m_k m_k^T.\n",
        "$$\n",
        "\n",
        "Sum over k=1,2. Since all rows of X are all samples, we have\n",
        "$$\n",
        "\\sum_{x\\in C_1}xx^T + \\sum_{x\\in C_2}xx^T = X^T X.\n",
        "$$\n",
        "\n",
        "Therefore\n",
        "$$\n",
        "\\boxed{\n",
        "S_w = X^T X - n_1 m_1 m_1^T - n_2 m_2 m_2^T.\n",
        "}\n",
        "$$\n"
      ],
      "id": "SVcyMjmCRnJ2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Y2ZAfkRnJ2"
      },
      "source": [
        "### 4)\n",
        "\n",
        "Assume centered data: $n_1m_1+n_2m_2=0$\n",
        "\n",
        "Let $d=m_1-m_2$\n",
        "\n",
        "**(a): Express means in terms of $d$**\n",
        "\n",
        "From $n_1m_1=-n_2m_2$, we get $m_2 = -\\frac{n_1}{n_2}m_1$. Substituting into $d = m_1 - m_2$:\n",
        "\n",
        "$$\n",
        "d = m_1 + \\tfrac{n_1}{n_2}m_1 = \\tfrac{n_1+n_2}{n_2}m_1\n",
        "\\quad\\Longrightarrow\\quad\n",
        "m_1=\\frac{n_2}{n_1+n_2}d, \\text{ similarly: } m_2=-\\frac{n_1}{n_1+n_2}d.\n",
        "$$\n",
        "\n",
        "Now substitute into the weighted sum of outer products:\n",
        "\n",
        "$$\n",
        "n_1m_1m_1^T+n_2m_2m_2^T\n",
        "= n_1\\frac{n_2^2}{(n_1+n_2)^2}dd^T + n_2\\frac{n_1^2}{(n_1+n_2)^2}dd^T\n",
        "= \\frac{n_1 n_2(n_2 + n_1)}{(n_1+n_2)^2}dd^T\n",
        "= \\frac{n_1n_2}{n_1+n_2}dd^T.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{n_1m_1m_1^T+n_2m_2m_2^T = \\frac{n_1n_2}{n_1+n_2}(m_1-m_2)(m_1-m_2)^T.}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "**(b): Show $X^Ty = n_1m_1 - n_2m_2 = \\frac{2n_1n_2}{n_1+n_2}(m_1 - m_2)$.**\n",
        "\n",
        "Expand $X^Ty$ using the definition of the label vector:\n",
        "\n",
        "$$\n",
        "X^Ty = \\sum_{i=1}^n y_i x_i\n",
        "= \\sum_{x\\in C_1}(+1)\\cdot x + \\sum_{x\\in C_2}(-1)\\cdot x\n",
        "= n_1m_1-n_2m_2.\n",
        "$$\n",
        "\n",
        "Now substitute the expressions for $m_1$ and $m_2$ from (a):\n",
        "\n",
        "$$\n",
        "n_1m_1 - n_2m_2\n",
        "= n_1\\cdot\\frac{n_2}{n_1+n_2}d \\;-\\; n_2\\cdot\\!\\left(-\\frac{n_1}{n_1+n_2}d\\right)\n",
        "= \\frac{n_1 n_2}{n_1+n_2}d + \\frac{n_1 n_2}{n_1+n_2}d\n",
        "= \\frac{2n_1n_2}{n_1+n_2}d.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{X^Ty = \\frac{2n_1n_2}{n_1+n_2}(m_1-m_2).}\n",
        "$$"
      ],
      "id": "1-Y2ZAfkRnJ2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm4ezFpbRnJ3"
      },
      "source": [
        "## 5)\n",
        "\n",
        "**Step 1: Show $S_w + cS_b = X^TX$ where $c = \\frac{n_1n_2}{n_1+n_2}$.**\n",
        "\n",
        "From Problem 3: $S_w = X^TX - n_1m_1m_1^T - n_2m_2m_2^T$.\n",
        "\n",
        "From Problem 4(a): $n_1m_1m_1^T + n_2m_2m_2^T = cS_b$.\n",
        "\n",
        "Substituting:\n",
        "\n",
        "$$\n",
        "S_w = X^TX - cS_b\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_w + cS_b = X^TX\n",
        "$$\n",
        "\n",
        "**Step 2: Show the two Rayleigh quotients have the same maximizer.**\n",
        "\n",
        "The original Fisher objective is:\n",
        "\n",
        "$$\n",
        "w^*=\\arg\\max_w \\frac{w^TS_bw}{w^TS_ww}.\n",
        "$$\n",
        "\n",
        "Define $A(w)=w^TS_bw\\ge 0$ and $B(w)=w^TS_ww>0$. Since $w^TX^TXw = B(w)+cA(w)$:\n",
        "\n",
        "$$\n",
        "\\frac{A(w)}{w^TX^TXw} = \\frac{A(w)}{B(w)+cA(w)}\n",
        "= \\frac{A(w)/B(w)}{1+c\\,A(w)/B(w)}.\n",
        "$$\n",
        "\n",
        "The function $f(t) = t/(1+ct)$ is strictly increasing for $t\\ge 0$ (its derivative is $1/(1+ct)^2 > 0$). So maximizing $A/B$ is equivalent to maximizing $f(A/B) = A/(B+cA)$:\n",
        "\n",
        "$$\n",
        "w^*=\\arg\\max_w \\frac{w^TS_bw}{w^T(X^TX)w}.\n",
        "$$\n",
        "\n",
        "**Step 3: Constrained form**\n",
        "\n",
        "This ratio is scale-invariant ($w$ and $\\alpha w$ give the same value for any $\\alpha\\neq 0$), so we can fix the denominator to 1:\n",
        "\n",
        "$$\n",
        "\\boxed{w^*=\\arg\\max_w\\; w^TS_bw\\quad \\text{s.t.}\\quad w^TX^TXw=1.}\n",
        "$$"
      ],
      "id": "sm4ezFpbRnJ3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xGvUuhoRnJ3"
      },
      "source": [
        "### 6) Show that $X^TX$ is invertible\n",
        "\n",
        "Assume $X\\in\\mathbb{R}^{n\\times p}$ has full column rank $p$.\n",
        "\n",
        "This means its $p$ columns are linearly independent, which is equivalent to\n",
        "\n",
        "$$\n",
        "\\mathcal{N}(X) = \\{\\, w \\in \\mathbb{R}^p : Xw = 0 \\,\\}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Now take any nonzero $w\\in\\mathbb{R}^p (w\\neq 0)$. Since $\\mathcal{N}(X)=\\{0\\}$, we must have\n",
        "$$\n",
        "Xw \\neq 0.\n",
        "$$\n",
        "\n",
        "Compute:\n",
        "$$\n",
        "w^T X^T X w = (Xw)^T (Xw).\n",
        "$$\n",
        "Because $Xw$ is a nonzero vector, its squared Euclidean norm is strictly positive:\n",
        "$$\n",
        "(Xw)^T (Xw) = |Xw|_2^2 > 0.\n",
        "$$\n",
        "\n",
        "Therefore, for every nonzero w,\n",
        "$$\n",
        "w^T X^T X w > 0.\n",
        "$$\n",
        "So $X^T X$ is positive definite, hence it has no nonzero vector in its null space:\n",
        "if $(X^T X)w=0$, then premultiplying by $w^T$ gives $w^T X^T X w = 0$, which contradicts the strict positivity unless $w=0$. Therefore\n",
        "$$\n",
        "\\mathcal{N}(X^T X)={0},\n",
        "$$\n",
        "so $X^T X$ is invertible."
      ],
      "id": "1xGvUuhoRnJ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Lagrangian critical points are eigenvectors\n",
        "\n",
        "From Problem (5), the constrained optimization is\n",
        "$$\n",
        "\\max_{w\\neq 0}\\; w^T S_b w\n",
        "\\quad\\text{s.t.}\\quad\n",
        "w^T X^T X\\, w = 1\n",
        "$$\n",
        "\n",
        "Form the Lagrangian\n",
        "\n",
        "$$\n",
        "L(w,\\lambda)=w^T S_b w-\\lambda\\big(w^T X^T X\\, w-1\\big).\n",
        "$$\n",
        "\n",
        "At any critical point $(w^*,\\lambda^*)$, we must have:\n",
        "$$\n",
        "\\text{(i) stationarity:}\\quad \\nabla_w L(w^*,\\lambda^*)=0,\n",
        "\\qquad\n",
        "\\text{(ii) feasibility:}\\quad (w^*)^T X^T X\\, w^* = 1.\n",
        "$$\n",
        "\n",
        "Using the standard identity\n",
        "$$\n",
        "\\nabla_w(w^T A w) = (A + A^T)w,\n",
        "$$\n",
        "and noting that $S_b$ and $X^T X$ are symmetric (so $A^T=A$), we have\n",
        "$$\n",
        "\\nabla_w(w^T A w)=2Aw.\n",
        "$$\n",
        "\n",
        "\n",
        "stationarity gives\n",
        "$$\n",
        "2S_b w^* - 2\\lambda^* X^T X\\, w^* = 0\n",
        "\\quad\\Longrightarrow\\quad\n",
        "S_b w^* = \\lambda^* X^T X\\, w^*.\n",
        "$$\n",
        "\n",
        "By Problem (6), $X^T X$ is invertible, so this is equivalent to\n",
        "$$\n",
        "(X^T X)^{-1} S_b\\, w^* = \\lambda^* w^*.\n",
        "$$\n",
        "Let $\\mu=\\lambda^*$. Then $(X^T X)^{-1}S_b w^*=\\mu w^*$.\n",
        "Hence every critical point $w^*$ is an eigenvector of $(X^T X)^{-1}S_b$ with eigenvalue $\\mu$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5TXmpjXvZIJ"
      },
      "id": "Q5TXmpjXvZIJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg1aNAMORnJ3"
      },
      "source": [
        "### 8) Is a product of symmetric matrices symmetric?\n",
        "\n",
        "No. If A and B are symmetric, then\n",
        "$$\n",
        "(AB)^T = B^T A^T = BA.\n",
        "$$\n",
        "\n",
        "Therefore,\n",
        "$$\n",
        "AB \\text{ is symmetric } \\iff (AB)^T = AB \\iff BA = AB,\n",
        "$$\n",
        "\n",
        "\n",
        "**Counterexample:**\n",
        "\n",
        "Let\n",
        "\n",
        "$$\n",
        "A=\\begin{bmatrix}1&1\\\\1&0\\end{bmatrix},\\qquad\n",
        "B=\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Both are symmetric, but:\n",
        "\n",
        "$$\n",
        "AB=\\begin{bmatrix}1&0\\\\1&0\\end{bmatrix},\\qquad\n",
        "(AB)^T=\\begin{bmatrix}1&1\\\\0&0\\end{bmatrix}\\neq AB.\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "Cg1aNAMORnJ3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsgDVfWwRnJ3"
      },
      "source": [
        "Let $d=m_1-m_2$ and $S_b=dd^T$. Define\n",
        "$$\n",
        "M=(X^TX)^{-1}S_b=(X^TX)^{-1}dd^T.\n",
        "$$\n",
        "\n",
        "**If $d\\neq 0$, then $\\operatorname{rank}(dd^T)=1$**\n",
        "\n",
        "For any $v\\in\\mathbb{R}^p$,\n",
        "$$\n",
        "(dd^T)v = d(d^T v),\n",
        "$$\n",
        "which is always a scalar multiple of $d$, hence\n",
        "$$\n",
        "\\mathrm{Range}(dd^T)\\subseteq \\mathrm{span}\\{d\\}.\n",
        "$$\n",
        "Taking $v=d$ gives\n",
        "$$\n",
        "(dd^T)d = d(d^T d) = (d^T d)\\,d \\neq 0,\n",
        "$$\n",
        "so $\\mathrm{Range}(dd^T)$ is not $\\{0\\}$ and therefore\n",
        "$$\n",
        "\\mathrm{Range}(dd^T)=\\mathrm{span}\\{d\\},\n",
        "$$\n",
        "which is 1-dimensional. Thus $\\operatorname{rank}(dd^T)=1$.\n",
        "\n",
        "**Since $X^TX$ is invertible, $(X^TX)^{-1}$ is invertible, and left-multiplication by an invertible matrix preserves rank,**\n",
        "$$\n",
        "\\operatorname{rank}(M)=\\operatorname{rank}\\big((X^TX)^{-1}dd^T\\big)=\\operatorname{rank}(dd^T)=1.\n",
        "$$\n",
        "\n",
        "By the rank-nullity theorem on $\\mathbb{R}^p$,\n",
        "$$\n",
        "\\dim\\mathcal{N}(M)=p-\\operatorname{rank}(M)=p-1.\n",
        "$$"
      ],
      "id": "dsgDVfWwRnJ3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj254HWGRnJ3"
      },
      "source": [
        "## 10)\n",
        "\n",
        "We want to solve:\n",
        "$$\n",
        "w_{OLS}=\\arg\\min_w\\|y-Xw\\|^2.\n",
        "$$\n",
        "\n",
        "Let\n",
        "$$\n",
        "f(w)=\\|y-Xw\\|^2=(y-Xw)^T(y-Xw).\n",
        "$$\n",
        "Taking the gradient with respect to $w$ and setting it to zero gives\n",
        "$$\n",
        "\\nabla_w f(w)=-2X^T(y-Xw)=0.\n",
        "$$\n",
        "Therefore,\n",
        "$$\n",
        "X^TXw_{OLS}=X^Ty,\n",
        "$$\n",
        "and since $X^T X$ is invertible,\n",
        "$$\n",
        "w_{OLS}=(X^T X)^{-1}X^T y.\n",
        "$$\n",
        "\n",
        "Using the identity from 4b,\n",
        "\n",
        "$$\n",
        "X^T y = n_1m_1 - n_2m_2,\n",
        "$$\n",
        "so\n",
        "$$\n",
        "\\boxed{w_{OLS}=(X^T X)^{-1}(n_1m_1-n_2m_2)}\n",
        "$$"
      ],
      "id": "bj254HWGRnJ3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CgQvdG7RnJ3"
      },
      "source": [
        "### 11) $w_{OLS}$ is an eigenvector of $(X^TX)^{-1}S_b$\n",
        "\n",
        "Assume $n_1m_1+n_2m_2=0$, so by (10) and (4b)\n",
        "\n",
        "$$\n",
        "w_{OLS}=(X^TX)^{-1}X^Ty=\\frac{2n_1n_2}{n_1+n_2}(X^TX)^{-1}(m_1-m_2)\n",
        "$$\n",
        "\n",
        "Let\n",
        "$$\n",
        "d=m_1-m_2,\\qquad v=(X^TX)^{-1}d,\\qquad \\alpha=\\frac{2n_1n_2}{n_1+n_2}\n",
        "$$\n",
        "so that\n",
        "$$\n",
        "w_{OLS}=\\alpha v\n",
        "$$\n",
        "\n",
        "Apply $(X^TX)^{-1}S_b$ to $w_{OLS}$:\n",
        "\n",
        "$$\n",
        "(X^TX)^{-1}S_b,w_{OLS}\n",
        "=(X^TX)^{-1}\\underbrace{dd^T}{S_b}(\\alpha v)\n",
        "=\\alpha,(X^TX)^{-1}d,\\underbrace{(d^Tv)}{\\text{scalar}}\n",
        "=\\alpha,(d^Tv),\\underbrace{(X^TX)^{-1}d}{=v}\n",
        "=(d^Tv),(\\alpha v)\n",
        "=(d^Tv),w{OLS}\n",
        "$$\n",
        "\n",
        "Hence $w_{OLS}$ is an eigenvector of $(X^TX)^{-1}S_b$ with eigenvalue\n",
        "$$\n",
        "\\mu=d^Tv=d^T(X^TX)^{-1}d.\n",
        "$$\n",
        "If $d\\neq 0$, then $\\mu>0$ because $(X^TX)^{-1}$ is positive definite.\n"
      ],
      "id": "_CgQvdG7RnJ3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_KQo89RnJ4"
      },
      "source": [
        "### 12) Uniqueness of Fisher direction\n",
        "\n",
        "From Problem 7, any Fisher optimum $w^*$ must be an eigenvector of\n",
        "$$\n",
        "M = (X^TX)^{-1}S_b\n",
        "$$\n",
        "\n",
        "From Problem 9, $\\operatorname{rank}(M)=1$, so $\\operatorname{col}(M)$ is 1-dimensional.\n",
        "\n",
        "If $Mw=\\mu w$ with $\\mu\\neq 0$, then\n",
        "$$\n",
        "w=\\frac{1}{\\mu}Mw\\in \\operatorname{col}(M)\n",
        "$$\n",
        "so there is at most one linearly independent eigenvector direction with nonzero eigenvalue.\n",
        "\n",
        "Assume $d=m_1-m_2\\neq 0$,\n",
        "\n",
        "Then $S_b=dd^T\\neq 0$,\n",
        "\n",
        "and the direction $w=(X^TX)^{-1}d$ gives\n",
        "\n",
        "$$\n",
        "w^T S_b w = w^Tdd^Tw = (d^Tw)^2>0,\n",
        "$$\n",
        "\n",
        "so the Fisher optimum cannot lie in $\\mathcal{N}(M)$ (which would yield $w^TS_bw=0$).\n",
        "\n",
        "Hence the Fisher optimum must lie in the unique 1D nonzero-eigendirection\n",
        "\n",
        "$$\n",
        "\\operatorname{col}(M)=\\operatorname{span}\\{(X^TX)^{-1}d\\}.\n",
        "$$\n",
        "\n",
        "From Problem 11, $w_{OLS}$ is an eigenvector of $M$ with eigenvalue\n",
        "\n",
        "$$\n",
        "\\mu=d^T(X^TX)^{-1}d>0,\n",
        "$$\n",
        "\n",
        "so $w_{OLS}\\in \\operatorname{col}(M)$ and spans this unique nonzero direction.\n",
        "\n",
        "Therefore the Fisher discriminant direction is unique (up to scaling and sign) and coincides with the OLS direction:\n",
        "$$\n",
        "\\boxed{w^* \\propto w_{OLS} \\propto (X^TX)^{-1}(m_1-m_2)}\n",
        "$$"
      ],
      "id": "qS_KQo89RnJ4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}